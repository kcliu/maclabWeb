
<div class="container">
  <div class="content"><br/>
    <div class="jumbotron">
      <p>The Music and Audio Computing &lpar;MAC&rpar; Lab of the Academia Sinica in Taipei&comma; part of its Research Center for Information Technology Innovation&comma; was founded in Sept&period; 2011 by Dr&period; Yi-Hsuan Yang&period; The MAC Lab is dedicated to the development of multimedia systems that better understand what we hear and perceive in sounds and apply this understanding to enhance our interaction with sounds&period; Our goal is to carry out highly original and competitive research at the international level and transfer the technology to impact the daily life&period;</p>
      <p>Our current research interests include&comma; but are not limited to&colon; music emotion recognition&comma; automatic tagging&comma; music recommendation&comma; multi-pitch estimation&comma; lyrics processing&comma; auditory scene analysis&comma; audio event detection&comma; singer&sol;speaker identification&comma; audio fingerprinting&comma; human affect detection&comma; and user interface design for mobile devices&period;</p>
    </div>
    <h1>Reseach Projects</h1><br/>
    <ul class="media-list">
      <li class="media"><a class="pull-left"><img src="./images/emo_clas.jpg" height="320" width="320" class="media-object img-thumbnail"/></a>
        <div class="media-body">
          <h4 class="media-heading">Large-Sacle music emotion classification</h4>Emotion-based music retrieval has been considered as a prominent way of music information access in the digital era&comma; as music is often created to convey emotion&period; This project aims at building a practical emotion classification system using large-scale data sets and a fine-grained level of emotion description&period;  Specifically&comma; we are interested in exploiting the rich repository of online tags to create a classification system that discriminates up to hundreds of emotions&comma; just in corresponding to the richness of emotion perception we have while listening to music&period; This project would involve various machine learning techniques such as multi-label classification&comma; cost-sensitive learning&comma; multiple instance learning&comma; and structured learning&period;
        </div>
      </li>
      <li class="media"><a class="pull-left"><img src="./images/emoRecom.jpg" height="320" width="320" class="media-object img-thumbnail"/></a>
        <div class="media-body">
          <h4 class="media-heading">Context-aware music recommendation on mobile devices</h4>This project aims at understanding the interplay between listening context&comma; listening mood&comma; and music content&period; Specifically&comma; the envisioned system is a user-centered system which learns to recommend music to a specific user according to day-to-day interaction with the user&period; We are interested in modeling the user&rsquor;s listening behavior according to listening profile&comma; but also interpreting the momentary listening context &lpar;time&comma; location&comma; activity&rpar; and listening mood &lpar;via face&comma; sound&comma; or physiological signals&rpar; by multimodal sensors&comma; especially those deployed on mobile devices&period; This multidisciplinary-natured research would place more emphasis on user modeling&period;
        </div>
      </li>
      <li class="media"><a class="pull-left"><img src="./images/sparseRepre.jpg" height="320" width="320" class="media-object img-thumbnail"/></a>
        <div class="media-body">
          <h4 class="media-heading">Sparse representation of music signals and source separation</h4>This project involves the extraction of multiple pitch contours from monaural music signals using an over-complete dictionary of music atoms and optimization techniques that solve a sparse representation problem&comma; mostly based on L1 minimization&period; We are keen to understand which feature representation of music and which decomposition techniques of the representation bring about the best extraction performance&period; Envisioned applications include multi-pitch extraction&comma; singer identification&comma; audio source separation&comma; etc&period; We expect more mathematics and digital signal processing in this project&period;
        </div>
      </li>
    </ul>
  </div>
</div>